import asyncio
import json
import time
from pathlib import Path
from typing import List, Dict, Optional, Any

from playwright.async_api import Page

from agio.components.tools.builtin_tools.adapter import SettingsRegistry
from agio.components.tools.builtin_tools.adapter import AppSettings
from agio.components.tools.builtin_tools.adapter import get_logger
from agio.components.tools.builtin_tools.web_fetch_tool.html_extract import (
    HtmlContent,
    extract_content_from_html,
)
from agio.components.tools.builtin_tools.web_fetch_tool.playwright.exceptions import (
    SessionInvalidException,
    BlockedException,
)
from agio.components.tools.builtin_tools.web_fetch_tool.playwright.chrome_session import (
    ChromeSessionManager,
)


# ==================== æ ¸å¿ƒçˆ¬è™«ç±» ====================
class PlaywrightCrawler:
    """ç”Ÿäº§çº§çˆ¬è™«"""

    def __init__(self, settings: Optional[AppSettings] = None):
        self.logger = get_logger(__name__)
        self._settings = settings or SettingsRegistry.get()
        self.session_manager: Optional[ChromeSessionManager] = None
        self._start_lock = asyncio.Lock()
        self._started = False
        self.site_configs: Dict[str, Dict[str, Any]] = {
            "wechat": {
                "login_url": "https://mp.weixin.qq.com/",
                "content_selectors": ["#js_content", ".rich_media_content"],
                "title_selectors": ["#activity-name", ".rich_media_title"],
                "auth_indicators": [".user_info", ".account_meta_value"],
                "name": "å¾®ä¿¡å…¬ä¼—å·",
            },
            "zhihu": {
                "login_url": "https://www.zhihu.com/",
                "content_selectors": [".RichContent-inner", ".Post-RichText"],
                "title_selectors": [".QuestionHeader-title", ".Post-Title"],
                "auth_indicators": [".AppHeader-userInfo", ".Avatar"],
                "name": "çŸ¥ä¹",
            },
            "weibo": {
                "login_url": "https://weibo.com/",
                "content_selectors": [".WB_text", ".WB_detail"],
                "title_selectors": [".WB_text", ".WB_info"],
                "auth_indicators": [".gn_name", ".username"],
                "name": "å¾®åš",
            },
        }

        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "blocked_requests": 0,
            "start_time": None,
        }

    async def __aenter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.stop()

    async def start(self):
        """å¯åŠ¨çˆ¬è™«ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œç¡®ä¿åªå¯åŠ¨ä¸€æ¬¡ï¼‰"""
        async with self._start_lock:
            # å¦‚æœå·²ç»å¯åŠ¨ï¼Œç›´æ¥è¿”å›
            if (
                self._started
                and self.session_manager
                and self.session_manager.is_connected()
            ):
                return

            self.stats["start_time"] = time.time()
            self.logger.info("ğŸš€ å¯åŠ¨ç”Ÿäº§çº§çˆ¬è™«")
            if not self.session_manager:
                self.session_manager = ChromeSessionManager(settings=self._settings)

            # è¿æ¥åˆ°Chrome
            await self.session_manager.connect()
            self._started = True

    async def stop(self):
        """åœæ­¢çˆ¬è™«"""
        async with self._start_lock:
            if not self._started:
                return

            self.logger.info("ğŸ›‘ åœæ­¢çˆ¬è™«")

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self._print_stats()

            # æ–­å¼€è¿æ¥
            if self.session_manager:
                await self.session_manager.disconnect()

            self._started = False

    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.stats["start_time"]:
            return

        duration = time.time() - self.stats["start_time"]
        success_rate = (
            (self.stats["successful_requests"] / self.stats["total_requests"] * 100)
            if self.stats["total_requests"] > 0
            else 0
        )

        self.logger.info(
            f"\nğŸ“Š çˆ¬è™«ç»Ÿè®¡:\n"
            f"   æ€»è¯·æ±‚: {self.stats['total_requests']}\n"
            f"   æˆåŠŸ: {self.stats['successful_requests']}\n"
            f"   å¤±è´¥: {self.stats['failed_requests']}\n"
            f"   è¢«æ‹¦æˆª: {self.stats['blocked_requests']}\n"
            f"   æˆåŠŸç‡: {success_rate:.1f}%\n"
            f"   è¿è¡Œæ—¶é—´: {duration:.1f}s"
        )

    def _is_blocked(self, page: Page) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª"""
        url = page.url.lower()
        blocked_keywords = [
            "captcha",
            "verify",
            "validation",
            "robots",
            "checkpoint",
            "challenge",
            "recaptcha",
        ]
        return any(keyword in url for keyword in blocked_keywords)

    async def _extract_content(self, page: Page, url: str) -> HtmlContent | None:
        """æå–é¡µé¢å†…å®¹"""

        # FIXME use Trafilatura to extract content
        original_html = await page.content()

        content: HtmlContent = extract_content_from_html(
            html=original_html, original_url=url
        )
        if not content:
            return None
        return content

    async def crawl_url(self, url: str, retries: int = 0) -> HtmlContent | None:
        """
        çˆ¬å–å•ä¸ªURL

        Args:
            url: ç›®æ ‡URL
            retries: é‡è¯•æ¬¡æ•°

        Returns:
            æå–çš„å†…å®¹æ•°æ®æˆ–None
        """
        if not self.session_manager.is_connected():
            raise SessionInvalidException("æœªè¿æ¥åˆ°Chrome")

        self.stats["total_requests"] += 1
        self.logger.info(f"æ­£åœ¨çˆ¬å–: {url}")
        try:
            page = await self.session_manager.context.new_page()
            # è®¾ç½®é¡µé¢è¶…æ—¶
            page.set_default_timeout(
                self._settings.tool.web_fetch_tool_timeout_seconds * 1000
            )

            # è®¿é—®é¡µé¢
            response = await page.goto(
                url, wait_until=self._settings.tool.web_fetch_wait_strategy
            )

            if not response or response.status != 200:
                self.logger.warning(
                    f"HTTPçŠ¶æ€å¼‚å¸¸: {response.status if response else 'None'}"
                )

                self.stats["failed_requests"] += 1
                return None

            # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            if self._is_blocked(page):
                self.logger.warning(f"ğŸš« è¯·æ±‚è¢«æ‹¦æˆª: {page.url}")
                self.stats["blocked_requests"] += 1
                raise BlockedException("è¯·æ±‚è¢«æ‹¦æˆª")

            # æå–å†…å®¹
            content = await self._extract_content(page, url)
            self.stats["successful_requests"] += 1
            self.logger.info(
                f"âœ… çˆ¬å–æˆåŠŸ: title:{content.title if content else 'None'}", url=url
            )

            return content

        except BlockedException:
            if retries < self._settings.tool.web_fetch_max_retries:
                self.logger.info(
                    f"é‡è¯• {retries + 1}/{self._settings.tool.web_fetch_max_retries}"
                )
                await asyncio.sleep(5 * (retries + 1))  # æŒ‡æ•°é€€é¿
                return await self.crawl_url(url, retries + 1)

        except Exception as e:
            self.logger.error(f"çˆ¬å–å¤±è´¥: {e}", url=url)
            self.stats["failed_requests"] += 1

            # å¥åº·æ£€æŸ¥ï¼Œå¦‚æœè¿æ¥æ–­å¼€åˆ™é‡è¿
            if not await self.session_manager.health_check():
                self.logger.info("æ£€æµ‹åˆ°è¿æ¥æ–­å¼€ï¼Œå°è¯•é‡è¿...")
                await self.session_manager.connect()

        finally:
            await page.close()

    async def crawl_batch(
        self, urls: List[str], save_dir: Optional[Path] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–URL

        Args:
            urls: URLåˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            æˆåŠŸçˆ¬å–çš„å†…å®¹åˆ—è¡¨
        """
        results = []
        save_dir = save_dir or Path("crawled_data")
        save_dir.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å–ï¼Œå…± {len(urls)} ä¸ªURL")

        for i, url in enumerate(urls, 1):
            self.logger.info(f"\n[{i}/{len(urls)}] å¤„ç† {url}")

            try:
                content = await self.crawl_url(
                    url, retries=self._settings.tool.web_fetch_max_retries
                )
                if content:
                    results.append(content)

                    # ä¿å­˜åˆ°æ–‡ä»¶
                    file_name = f"{content['timestamp']}_{hash(url)}.json"
                    (save_dir / file_name).write_text(
                        json.dumps(content, ensure_ascii=False, indent=2),
                        encoding="utf-8",
                    )
            except Exception as e:
                self.logger.error(f"å¤„ç†URLå¤±è´¥: {e}")

        self.logger.info(f"æ‰¹é‡çˆ¬å–å®Œæˆï¼ŒæˆåŠŸ {len(results)}/{len(urls)}")
        return results
